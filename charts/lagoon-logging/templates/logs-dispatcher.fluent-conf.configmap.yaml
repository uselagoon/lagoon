apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "lagoon-logging.logsDispatcher.fullname" . }}-fluent-conf
  labels:
    {{- include "lagoon-logging.logsDispatcher.labels" . | nindent 4 }}
data:
  fluent.conf: |
    # vi: ft=fluentd
    <system>
      workers 2
    </system>

    # prometheus metrics
    <source>
      @type prometheus
    </source>
    <source>
      @type prometheus_monitor
    </source>
    <source>
      @type prometheus_output_monitor
    </source>

    # container logs collected by the logging-operator
    <source>
      @type  forward
      @id    in_container
      tag    process.container
    </source>

    # application logs emitted by the lagoon_logs drupal module
    <source>
      @type  udp
      @id    in_application
      tag    "lagoon.#{ENV['CLUSTER_NAME']}.application"
      port   5140
      # max IPv4 UDP payload size
      message_length_limit 65507
      <parse>
        @type json
      </parse>
    </source>

    # router logs emitted by the openshift routers
    <source>
      @type                syslog
      @id                  in_router_openshift
      tag                  "lagoon.#{ENV['CLUSTER_NAME']}.router.openshift"
      emit_unmatched_lines true
      # syslog parameters
      port         5141
      severity_key severity
      # max IPv4 UDP payload size
      message_length_limit 65507
      <parse>
        @type       regexp
        # parse HTTP logs based on the haproxy documentation
        # As per the documentation here
        # https://www.haproxy.com/documentation/hapee/1-8r1/onepage/#8.2.3, except
        # we split the frontend_name into its constituent parts as used by
        # openshift.
        expression           /^.{,15} (?<process_name>\w+)\[(?<pid>\d+)\]: (?<client_ip>\S+):(?<client_port>\d+) \[(?<request_date>\S+)\] (?<frontend_name>\S+) (?<backend_type>\S+):(?<docker_container_id>(?<kubernetes_namespace_name>\S+):\S+\/pod:(?<kubernetes_pod_name>[^:]+):(?<kubernetes_container_name>[^:]+)):\S+ (?<TR>[\d-]+)\/(?<Tw>[\d-]+)\/(?<Tc>[\d-]+)\/(?<Tr>[\d-]+)\/(?<Ta>[\d-]+) (?<status_code>\d+) (?<bytes_read>\d+) (?<captured_request_cookie>\S+) (?<captured_response_cookie>\S+) (?<termination_state>\S+) (?<actconn>\d+)\/(?<feconn>\d+)\/(?<beconn>\d+)\/(?<srv_conn>\d+)\/(?<retries>\d+) (?<srv_queue>\d+)\/(?<backend_queue>\d+) "(?<http_request>.+)"/
        time_key             request_date
        time_format          %d/%b/%Y:%T.%L
        types                pid:integer,client_port:integer,TR:integer,Tw:integer,Tc:integer,Tr:integer,Ta:integer,bytes_read:integer,actconn:integer,feconn:integer,beconn:integer,srv_conn:integer,retries:integer,srv_queue:integer,backend_queue:integer
      </parse>
    </source>

    #
    # optional sources which can be enabled in the chart
    #
    @include source.d/*.conf

    #
    # capture unmatched openshift router logs
    #
    <filter lagoon.*.router.openshift.unmatched>
      @type record_modifier
      <record>
        index_name router-logs-openshift_parse_error_${ENV['CLUSTER_NAME']}-${Time.at(time).strftime("%Y.%m")}
      </record>
    </filter>

    #
    # pre-processing for nginx_router logs
    #
    # the reason for having the two match blocks is because we have two checks
    # to distinguish nginx_router logs:
    # * app label is "nginx-ingress"
    # * namespace is "syn-nginx-ingress"
    # if either of those checks fails the message is tagged as a regular
    # container log.
    #
    # check app name first. if app name didn't match, set tag to container log.
    <match process.container>
      @type rewrite_tag_filter
      <rule>
        key     $.kubernetes.labels.app
        pattern ^nginx-ingress$
        tag     "process.app_nginx_ingress"
      </rule>
      <rule>
        invert  true
        key     $.kubernetes.labels.app
        pattern ^nginx-ingress$
        tag     "lagoon.#{ENV['CLUSTER_NAME']}.container"
      </rule>
    </match>
    # check namespace_name. if it is okay too, tag as router log.
    # if namespace didn't match, set tag to container log.
    <match process.app_nginx_ingress>
      @type rewrite_tag_filter
      <rule>
        key     $.kubernetes.namespace_name
        pattern ^syn-nginx-ingress$
        tag     "lagoon.#{ENV['CLUSTER_NAME']}.router.nginx"
      </rule>
      <rule>
        invert  true
        key     $.kubernetes.namespace_name
        pattern ^syn-nginx-ingress$
        tag     "lagoon.#{ENV['CLUSTER_NAME']}.container"
      </rule>
    </match>

    #
    # process container logs
    #
    # restructure so the kubernetes_metadata plugin can find the keys it needs
    <filter lagoon.*.container>
      @type record_modifier
      remove_keys _dummy_
      <record>
        _dummy_ ${record['docker'] = {'container_id' => "#{record.dig('kubernetes','docker_id')}"}; nil}
      </record>
    </filter>
    # enrich with k8s metadata (will get the namespace labels)
    <filter lagoon.*.container>
      @type kubernetes_metadata
      @log_level warn
      skip_container_metadata true
      skip_master_url         true
    </filter>
    # strip the duplicate information so that it doesn't appear in logs
    <filter lagoon.*.container>
      @type record_modifier
      remove_keys docker
    </filter>
    # add the index name
    <filter lagoon.*.container>
      @type record_modifier
      <record>
        index_name container-logs-${record.dig('kubernetes','namespace_labels','lagoon_sh/project') || "#{record.dig('kubernetes','namespace_name') || 'unknown_project'}_#{ENV['CLUSTER_NAME']}"}-_-${record.dig('kubernetes','namespace_labels','lagoon_sh/environmentType') || "unknown_environmenttype"}-_-${Time.at(time).strftime("%Y.%m")}
      </record>
    </filter>
    # post-process to try to eke some more structure out of the logs.
    # the last "format none" block is a catch-all for unmatched messages.
    <filter lagoon.*.container>
      @type parser
      key_name log
      reserve_data true
      <parse>
        @type multi_format
        <pattern>
          format nginx
          types  size:integer
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>
    # some container logs have a duplicate message field for some reason, so
    # remove that.
    <filter lagoon.*.container>
      @type record_modifier
      remove_keys message
    </filter>

    #
    # process application logs
    #
    # restructure so the kubernetes_metadata plugin can find the keys it needs
    <filter lagoon.*.application>
      @type record_modifier
      remove_keys _dummy_,type
      <record>
        _dummy_ ${record['openshift_project'] = record['type']; record['kubernetes'] = {'namespace_name' => record['type'], 'pod_name' => record['host'], 'container_name' => 'unknown'}; record['docker'] = {'container_id' => "#{record['type']}_#{record['host']}"}; nil}
      </record>
    </filter>
    # enrich with k8s metadata (will get the namespace labels)
    <filter lagoon.*.application>
      @type kubernetes_metadata
      @log_level warn
      skip_container_metadata true
      skip_master_url         true
    </filter>
    # add the index_name
    <filter lagoon.*.application>
      @type record_modifier
      <record>
        index_name application-logs-${record.dig('kubernetes','namespace_labels','lagoon_sh/project') || "#{record.dig('kubernetes','namespace_name') || 'unknown_project'}_#{ENV['CLUSTER_NAME']}"}-_-${record.dig('kubernetes','namespace_labels','lagoon_sh/environmentType') || "unknown_environmenttype"}-_-${Time.at(time).strftime("%Y.%m")}
      </record>
    </filter>
    # strip the kubernetes data as it's duplicated in container/router logs and
    # not really relevant for application logs
    <filter lagoon.*.application>
      @type record_modifier
      remove_keys docker,kubernetes
    </filter>

    #
    # process nginx_router logs
    #
    # Strip the nginx-ingress namespace info and add enough dummy information
    # so that kubernetes_metadata plugin can get the namespace labels.
    # Also strip the duplicated log field.
    <filter lagoon.*.router.nginx>
      @type record_modifier
      remove_keys _dummy_,log
      <record>
        _dummy_ ${record['kubernetes'] = {'namespace_name' => record['namespace'], 'pod_name' => 'nopod', 'container_name' => 'nocontainer'}; record['docker'] = {'container_id' => "#{record['namespace']}_#{record['ingress_name']}"}; nil}
      </record>
    </filter>
    # enrich with k8s metadata (will get the namespace labels)
    <filter lagoon.*.router.nginx>
      @type kubernetes_metadata
      @log_level warn
      skip_container_metadata true
      skip_master_url         true
    </filter>
    # strip the dummy information so that it doesn't appear in logs
    <filter lagoon.*.router.nginx>
      @type record_modifier
      remove_keys _dummy_,docker
      <record>
        _dummy_ ${record['kubernetes'].delete('pod_name'); record['kubernetes'].delete('container_name'); record['kubernetes'].delete('pod_id'); nil}
      </record>
    </filter>

    #
    # process openshift router logs
    #
    # retructure the record enough for the kubernetes_metadata plugin to get
    # namespace labels
    <filter lagoon.*.router.openshift>
      @type record_modifier
      remove_keys _dummy_,kubernetes_namespace_name,kubernetes_pod_name,kubernetes_container_name,docker_container_id
      <record>
        _dummy_ ${record['kubernetes'] = {'namespace_name' => record['kubernetes_namespace_name'], 'pod_name' => record['kubernetes_pod_name'], 'container_name' => record['kubernetes_container_name']}; record['docker'] = {'container_id' => record['docker_container_id']}; nil}
      </record>
    </filter>
    # enrich with k8s metadata
    <filter lagoon.*.router.openshift>
      @type kubernetes_metadata
      @log_level warn
      skip_container_metadata true
      skip_master_url         true
    </filter>

    #
    # add the router index_name
    #
    <filter lagoon.*.router.*>
      @type record_modifier
      <record>
        index_name router-logs-${record.dig('kubernetes','namespace_labels','lagoon_sh/project') || "#{record.dig('kubernetes','namespace_name') || 'unknown_project'}_#{ENV['CLUSTER_NAME']}"}-_-${record.dig('kubernetes','namespace_labels','lagoon_sh/environmentType') || "unknown_environmenttype"}-_-${Time.at(time).strftime("%Y.%m")}
      </record>
    </filter>

    #
    # add the lagoon index_name
    # the source for this tag is included when lagoonLogs.enabled is true
    #
    <filter lagoon.*.lagoon>
      @type record_modifier
      <record>
        index_name lagoon-logs-${record['project']}-_-all_environments-_-${Time.at(time).strftime("%Y.%m")}
      </record>
    </filter>

    #
    # forward all to logs-concentrator
    #
    <match lagoon.**>
      @type copy
      <store>
        @type forward
        @id out_forward
        # error out early
        verify_connection_at_startup true
        # tls
        transport tls
        tls_cert_path /fluentd/tls/ca.crt
        tls_client_cert_path /fluentd/tls/client.crt
        tls_client_private_key_path /fluentd/tls/client.key
        {{- with .Values.forward.tlsVerifyHostname }}
        tls_verify_hostname {{ . }}
        {{- end }}
        # endpoint
        <server>
          port "#{ENV['LOGS_FORWARD_HOST_PORT']}"
          host "#{ENV['LOGS_FORWARD_HOST']}"
          name "#{ENV['LOGS_FORWARD_HOSTNAME']}"
          username "#{ENV['LOGS_FORWARD_USERNAME']}"
          password "#{ENV['LOGS_FORWARD_PASSWORD']}"
        </server>
        # authentication
        <security>
          self_hostname "#{ENV['LOGS_FORWARD_SELF_HOSTNAME']}"
          shared_key "#{ENV['LOGS_FORWARD_SHARED_KEY']}"
        </security>
        # buffer chunks by tag
        <buffer tag>
          @type file
          path /fluentd/buffer/forward
          # buffer params (per worker)
          total_limit_size 8GB
          # flush params
          flush_thread_count 2
          overflow_action drop_oldest_chunk
        </buffer>
      </store>
      @include store.d/*.conf
    </match>
